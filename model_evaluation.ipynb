{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User path config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/output/amazon-reviews-data-with-response-phi3-prompt.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(output, model_response, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate model's recommendation performance using precision@k, recall@k, MRR, HR@10, and NDCG@10.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The ground truth output containing the actual items (comma-separated string).\n",
    "        model_response (str): The predicted items by the model (comma-separated string).\n",
    "        k (int): The number of top items to consider for precision@k, recall@k, etc.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing precision@k, recall@k, MRR, HR@10, and NDCG@10.\n",
    "    \"\"\"\n",
    "    # Parse the output and model_response strings into lists\n",
    "    actual_items = [item.strip() for item in output.split(\",\") if \"item_\" in item]\n",
    "    predicted_items = [item.strip() for item in model_response.split(\",\") if \"item_\" in item][:k]  # Only consider top-K predictions\n",
    "\n",
    "    # Remove duplicates in predictions (since duplicates don't count multiple times)\n",
    "    predicted_items = list(dict.fromkeys(predicted_items))\n",
    "\n",
    "    # Calculate true positives\n",
    "    true_positives = set(actual_items) & set(predicted_items)\n",
    "    \n",
    "    # Calculate precision@k\n",
    "    precision_at_k = len(true_positives) / min(len(predicted_items), k)\n",
    "    \n",
    "    # Calculate recall@k\n",
    "    recall_at_k = len(true_positives) / len(actual_items) if len(actual_items) > 0 else 0\n",
    "    \n",
    "    # Calculate MRR (Mean Reciprocal Rank)\n",
    "    mrr = 0\n",
    "    for rank, predicted_item in enumerate(predicted_items, 1):\n",
    "        if predicted_item in actual_items:\n",
    "            mrr = 1 / rank\n",
    "            break\n",
    "    \n",
    "    # Calculate HR@10 (Hit Rate at 10)\n",
    "    hit_rate_at_k = 1 if len(true_positives) > 0 else 0\n",
    "\n",
    "    # Calculate NDCG@10 (Normalized Discounted Cumulative Gain)\n",
    "    dcg = 0\n",
    "    for i, item in enumerate(predicted_items):\n",
    "        if item in actual_items:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(min(len(actual_items), k))])\n",
    "    ndcg_at_k = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    # Compile metrics into a dictionary\n",
    "    metrics = {\n",
    "        f\"precision@{k}\": precision_at_k,\n",
    "        f\"recall@{k}\": recall_at_k,\n",
    "        \"mrr\": mrr,\n",
    "        f\"hr@{k}\": hit_rate_at_k,\n",
    "        f\"ndcg@{k}\": ndcg_at_k\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics_on_list(data_list, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate precision@k, recall@k, MRR, HR@10, and NDCG@10 on a list of data.\n",
    "    \n",
    "    Args:\n",
    "        data_list (list): A list of dictionaries containing 'output' and 'model_response'.\n",
    "        k (int): The number of top items to consider for precision@k, recall@k, etc.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Average precision@k, recall@k, MRR, HR@10, and NDCG@10 for the entire dataset.\n",
    "    \"\"\"\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    mrr_sum = 0\n",
    "    hr_sum = 0\n",
    "    ndcg_sum = 0\n",
    "    num_samples = len(data_list)\n",
    "\n",
    "    for data in data_list:\n",
    "        output = data[\"output\"]\n",
    "        model_response = data[\"model_response\"]\n",
    "\n",
    "        # Calculate metrics for each entry\n",
    "        metrics = evaluate_metrics(output, model_response, k)\n",
    "        \n",
    "        # Accumulate the metrics\n",
    "        precision_sum += metrics[f\"precision@{k}\"]\n",
    "        recall_sum += metrics[f\"recall@{k}\"]\n",
    "        mrr_sum += metrics[\"mrr\"]\n",
    "        hr_sum += metrics[f\"hr@{k}\"]\n",
    "        ndcg_sum += metrics[f\"ndcg@{k}\"]\n",
    "\n",
    "    # Calculate the average metrics over all samples\n",
    "    avg_metrics = {\n",
    "        f\"precision@{k}\": precision_sum / num_samples,\n",
    "        f\"recall@{k}\": recall_sum / num_samples,\n",
    "        \"mrr\": mrr_sum / num_samples,\n",
    "        f\"hr@{k}\": hr_sum / num_samples,\n",
    "        f\"ndcg@{k}\": ndcg_sum / num_samples\n",
    "    }\n",
    "\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_endoftext_items(data_list):\n",
    "    filtered_data = []\n",
    "    \n",
    "    for entry in data_list:\n",
    "        # Check if all tokens in 'output' are '<|endoftext|>'\n",
    "        output_items = [item.strip() for item in entry['output'].split(',')]\n",
    "        \n",
    "        # If the output is not all '<|endoftext|>', keep the entry\n",
    "        if any(item != '<|endoftext|>' for item in output_items):\n",
    "            filtered_data.append(entry)\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    data_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_with_atleast_one_item_output = filter_endoftext_items(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test data: 627\n",
      "Filtered test data with at least one output item: 230\n"
     ]
    }
   ],
   "source": [
    "print(\"Original test data:\", len(data_list))\n",
    "print(\"Filtered test data with at least one output item:\", len(data_list_with_atleast_one_item_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics: {'precision@10': 0.008040302777144884, 'recall@10': 0.01967038809144072, 'mrr': 0.015151515151515152}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the metrics\n",
    "avg_metrics = evaluate_metrics_on_list(data_list, k=10)\n",
    "print(\"Average Metrics:\", avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude materials from test data\n",
    "Exclude materials that do not have any item in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics: {'precision@10': 0.02191856452726018, 'recall@10': 0.0536231884057971, 'mrr': 0.041304347826086954}\n"
     ]
    }
   ],
   "source": [
    "avg_metrics = evaluate_metrics_on_list(data_list_with_atleast_one_item_output, k=10)\n",
    "print(\"Average Metrics:\", avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '<|user_AFIGEAZMENNV67CZRJH5P66MNIWA|>',\n",
       " 'output': '<|item_B07PJ8VDSP|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>',\n",
       " 'instruction': \"Given a user purchased an an item with the following details, predict the next 10 items the user would  purchase. Item id is <|item_B07TX13LTP|>. Rating of the item by user from 1 to 5 is 1.0. Text of the user review is Short life span . dont borther. Main category of the item is Appliances. Item name is DIKOO WR51X10101 Heater Harness Defrost Assembly Compatible for General Electric Refrigerators Replaces AP4355467, 1399613,EA1993872, WR51X10053. Price USD is 13.29. Item details is brand name is: DIKOO. model info is: WR51X10101DIK. item weight is: 7.8 ounces. package dimensions is: 9.96 x 3.98 x 3.43 inches. item model number is: WR51X10101DIK. part number is: WR51X10101DIK. form factor is: Compact. batteries included? is: No. batteries required? is: No. best sellers rank is: {'Tools & Home Improvement': 221564, 'Freezer Parts & Accessories': 212}. date first available is: July 6, 2019. brand is: DIKOO. pattern is: Solid.\",\n",
       " 'model_response': '<|item_B07TX13LTP|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07H3J23LJ|>, <|item_B07H3J23LJ|>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list_with_atleast_one_item_output[78]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
